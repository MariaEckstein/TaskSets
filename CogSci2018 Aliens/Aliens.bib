
@article{collins_reasoning_2012,
	title = {Reasoning, {Learning}, and {Creativity}: {Frontal} {Lobe} {Function} and {Human} {Decision}-{Making}},
	volume = {10},
	issn = {1545-7885},
	shorttitle = {Reasoning, {Learning}, and {Creativity}},
	doi = {10.1371/journal.pbio.1001293},
	abstract = {Computational modeling and behavioral experimentation suggest that human frontal lobe function is capable of monitoring three or four concurrent behavioral strategies in order to select the most suitable one during decision-making.},
	number = {3},
	journal = {PLOS Biology},
	author = {Collins, Anne GE and Koechlin, Etienne},
	year = {2012},
	keywords = {Behavior, Decision Making, Entropy, Human learning, Human performance, Learning, Long-term memory, Reliability},
	pages = {e1001293}
}

@article{tricomi_value_2015,
	title = {Value and probability coding in a feedback-based learning task utilizing food rewards},
	volume = {113},
	issn = {1522-1598},
	doi = {10.1152/jn.00086.2014},
	abstract = {For the consequences of our actions to guide behavior, the brain must represent different types of outcome-related information. For example, an outcome can be construed as negative because an expected reward was not delivered or because an outcome of low value was delivered. Thus behavioral consequences can differ in terms of the information they provide about outcome probability and value. We investigated the role of the striatum in processing probability-based and value-based negative feedback by training participants to associate cues with food rewards and then employing a selective satiety procedure to devalue one food outcome. Using functional magnetic resonance imaging, we examined brain activity related to receipt of expected rewards, receipt of devalued outcomes, omission of expected rewards, omission of devalued outcomes, and expected omissions of an outcome. Nucleus accumbens activation was greater for rewarding outcomes than devalued outcomes, but activity in this region did not correlate with the probability of reward receipt. Activation of the right caudate and putamen, however, was largest in response to rewarding outcomes relative to expected omissions of reward. The dorsal striatum (caudate and putamen) at the time of feedback also showed a parametric increase correlating with the trialwise probability of reward receipt. Our results suggest that the ventral striatum is sensitive to the motivational relevance, or subjective value, of the outcome, while the dorsal striatum codes for a more complex signal that incorporates reward probability. Value and probability information may be integrated in the dorsal striatum, to facilitate action planning and allocation of effort.},
	number = {1},
	journal = {Journal of Neurophysiology},
	author = {Tricomi, Elizabeth and Lempert, Karolina M.},
	year = {2015},
	pmid = {25339705},
	pmcid = {PMC4294573},
	keywords = {Anticipation, Psychological, Brain Mapping, caudate, Corpus Striatum, Cues, Feedback, Psychological, Female, fMRI, Food, Food Preferences, Humans, Magnetic Resonance Imaging, Male, Neuropsychological Tests, nucleus accumbens, Probability, Reward, reward processing, striatum, Young Adult},
	pages = {4--13}
}

@article{badre_mechanisms_2012,
	title = {Mechanisms of {Hierarchical} {Reinforcement} {Learning} in {Cortico}–{Striatal} {Circuits} 2: {Evidence} from {fMRI}},
	volume = {22},
	issn = {1047-3211, 1460-2199},
	shorttitle = {Mechanisms of {Hierarchical} {Reinforcement} {Learning} in {Cortico}–{Striatal} {Circuits} 2},
	doi = {10.1093/cercor/bhr117},
	abstract = {The frontal lobes may be organized hierarchically such that more rostral frontal regions modulate cognitive control operations in caudal regions. In our companion paper (Frank MJ, Badre D. 2011. Mechanisms of hierarchical reinforcement learning in corticostriatal circuits I: computational analysis. 22:509–526), we provide novel neural circuit and algorithmic models of hierarchical cognitive control in cortico–striatal circuits. Here, we test key model predictions using functional magnetic resonance imaging (fMRI). Our neural circuit model proposes that contextual representations in rostral frontal cortex influence the striatal gating of contextual representations in caudal frontal cortex. Reinforcement learning operates at each level, such that the system adaptively learns to gate higher order contextual information into rostral regions. Our algorithmic Bayesian “mixture of experts” model captures the key computations of this neural model and provides trial-by-trial estimates of the learner’s latent hypothesis states. In the present paper, we used these quantitative estimates to reanalyze fMRI data from a hierarchical reinforcement learning task reported in Badre D, Kayser AS, D’Esposito M. 2010. Frontal cortex and the discovery of abstract action rules. Neuron. 66:315--326. Results validate key predictions of the models and provide evidence for an individual cortico–striatal circuit for reinforcement learning of hierarchical structure at a specific level of policy abstraction. These findings are initially consistent with the proposal that hierarchical control in frontal cortex may emerge from interactions among nested cortico–striatal circuits at different levels of abstraction.},
	number = {3},
	journal = {Cerebral Cortex},
	author = {Badre, David and Frank, Michael J.},
	year = {2012},
	pmid = {21693491},
	keywords = {basal ganglia, cognitive control, fMRI, prefrontal cortex, reinforcement learning},
	pages = {527--536}
}

@article{frank_mechanisms_2012,
	title = {Mechanisms of {Hierarchical} {Reinforcement} {Learning} in {Cortico}-{Striatal} {Circuits} 1: {Computational} {Analysis}},
	volume = {22},
	issn = {1047-3211, 1460-2199},
	shorttitle = {Mechanisms of {Hierarchical} {Reinforcement} {Learning} in {Corticostriatal} {Circuits} 1},
	doi = {10.1093/cercor/bhr114},
	number = {3},
	journal = {Cerebral Cortex},
	author = {Frank, Michael J. and Badre, D.},
	year = {2012},
	pages = {509--526},
	file = {Frank_Badre_2011.pdf:C\:\\Users\\maria\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\a3wwptsq.default\\zotero\\storage\\ZWBVTQ34\\Frank_Badre_2011.pdf:application/pdf}
}

@book{marr_vision:_1982,
	address = {New York, NY, USA},
	title = {Vision: {A} {Computational} {Investigation} into the {Human} {Representation} and {Processing} of {Visual} {Information}},
	isbn = {978-0-7167-1567-2},
	shorttitle = {Vision},
	publisher = {Henry Holt and Co., Inc.},
	author = {Marr, David},
	year = {1982}
}

@article{daw_model-based_2011,
	title = {Model-{Based} {Influences} on {Humans}' {Choices} and {Striatal} {Prediction} {Errors}},
	volume = {69},
	issn = {08966273},
	doi = {10.1016/j.neuron.2011.02.027},
	number = {6},
	journal = {Neuron},
	author = {Daw, Nathaniel D. and Gershman, Samuel J. and Seymour, Ben and Dayan, Peter and Dolan, Raymond J.},
	year = {2011},
	pages = {1204--1215},
	file = {Daw2011_Initial2StepTask.pdf:C\:\\Users\\maria\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\a3wwptsq.default\\zotero\\storage\\S58W2WD5\\Daw2011_Initial2StepTask.pdf:application/pdf}
}

@book{sutton_reinforcement_2017,
	address = {Cambridge, MA; London, England},
	edition = {2},
	title = {Reinforcement {Learning}: {An} {Introduction}},
	publisher = {MIT Press},
	author = {Sutton, R. S. and Barto, A. G.},
	year = {2017},
	file = {Reinforcement Learning.pdf:C\:\\Users\\maria\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\a3wwptsq.default\\zotero\\storage\\G8UWF6N2\\Reinforcement Learning.pdf:application/pdf}
}

@article{collins_cognitive_2013,
	title = {Cognitive control over learning: creating, clustering, and generalizing task-set structure},
	volume = {120},
	issn = {1939-1471},
	shorttitle = {Cognitive control over learning},
	doi = {10.1037/a0030852},
	abstract = {Learning and executive functions such as task-switching share common neural substrates, notably prefrontal cortex and basal ganglia. Understanding how they interact requires studying how cognitive control facilitates learning but also how learning provides the (potentially hidden) structure, such as abstract rules or task-sets, needed for cognitive control. We investigate this question from 3 complementary angles. First, we develop a new context-task-set (C-TS) model, inspired by nonparametric Bayesian methods, specifying how the learner might infer hidden structure (hierarchical rules) and decide to reuse or create new structure in novel situations. Second, we develop a neurobiologically explicit network model to assess mechanisms of such structured learning in hierarchical frontal cortex and basal ganglia circuits. We systematically explore the link between these modeling levels across task demands. We find that the network provides an approximate implementation of high-level C-TS computations, with specific neural mechanisms modulating distinct C-TS parameters. Third, this synergism yields predictions about the nature of human optimal and suboptimal choices and response times during learning and task-switching. In particular, the models suggest that participants spontaneously build task-set structure into a learning problem when not cued to do so, which predicts positive and negative transfer in subsequent generalization tests. We provide experimental evidence for these predictions and show that C-TS provides a good quantitative fit to human sequences of choices. These findings implicate a strong tendency to interactively engage cognitive control and learning, resulting in structured abstract representations that afford generalization opportunities and, thus, potentially long-term rather than short-term optimality.},
	number = {1},
	journal = {Psychological Review},
	author = {Collins, Anne GE and Frank, Michael J},
	year = {2013},
	pmid = {23356780},
	pmcid = {PMC3974273},
	keywords = {Attention, basal ganglia, Bayes Theorem, COGNITION, Executive function, Frontal Lobe, Generalization (Psychology), Humans, Learning, Models, Psychological, Neural Networks (Computer), Reinforcement (Psychology)},
	pages = {190--229}
}

@article{miller_integrative_2001,
	title = {An integrative theory of prefrontal cortex function},
	volume = {24},
	issn = {0147-006X},
	doi = {10.1146/annurev.neuro.24.1.167},
	abstract = {The prefrontal cortex has long been suspected to play an important role in cognitive control, in the ability to orchestrate thought and action in accordance with internal goals. Its neural basis, however, has remained a mystery. Here, we propose that cognitive control stems from the active maintenance of patterns of activity in the prefrontal cortex that represent goals and the means to achieve them. They provide bias signals to other brain structures whose net effect is to guide the flow of activity along neural pathways that establish the proper mappings between inputs, internal states, and outputs needed to perform a given task. We review neurophysiological, neurobiological, neuroimaging, and computational studies that support this theory and discuss its implications as well as further issues to be addressed},
	language = {eng},
	journal = {Annual Review of Neuroscience},
	author = {Miller, E. K. and Cohen, J. D.},
	year = {2001},
	pmid = {11283309},
	keywords = {Animals, Attention, COGNITION, Humans, Memory, Models, Neurological, Neural Pathways, Neurons, Prefrontal Cortex},
	pages = {167--202}
}

@article{schultz_neural_1997,
	title = {A {Neural} {Substrate} of {Prediction} and {Reward}},
	volume = {275},
	copyright = {© 1997 American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	doi = {10.1126/science.275.5306.1593},
	abstract = {The capacity to predict future events permits a creature to detect, model, and manipulate the causal structure of its interactions with its environment. Behavioral experiments suggest that learning is driven by changes in the expectations about future salient events such as rewards and punishments. Physiological work has recently complemented these studies by identifying dopaminergic neurons in the primate whose fluctuating output apparently signals changes or errors in the predictions of future salient and rewarding events. Taken together, these findings can be understood through quantitative theories of adaptive optimizing control.},
	number = {5306},
	journal = {Science},
	author = {Schultz, Wolfram and Dayan, Peter and Montague, P. Read},
	year = {1997},
	pmid = {9054347},
	pages = {1593--1599},
	file = {Full Text PDF:C\:\\Users\\maria\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\a3wwptsq.default\\zotero\\storage\\HWSNFZZD\\Schultz et al. - 1997 - A Neural Substrate of Prediction and Reward.pdf:application/pdf;Snapshot:C\:\\Users\\maria\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\a3wwptsq.default\\zotero\\storage\\VEVNTEVS\\1593.html:text/html}
}

@article{donoso_foundations_2014,
	title = {Foundations of human reasoning in the prefrontal cortex},
	volume = {344},
	copyright = {Copyright © 2014, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	doi = {10.1126/science.1252254},
	abstract = {Selecting the most successful strategy
The brain's prefrontal cortex helps us to make decisions in an uncertain and constantly changing environment. Donoso et al. present a model of human reasoning as an algorithm implemented in the prefrontal cortex (see the Perspective by Hare). Brain-imaging experiments supported this model. Depending on the prevailing circumstances, human reasoning can either adapt ongoing behavioral strategies or switch to previously learned strategies. Only when neither approach is appropriate will the brain create new strategies.
Science, this issue p. 1481, see also p. 1446
The prefrontal cortex (PFC) subserves reasoning in the service of adaptive behavior. Little is known, however, about the architecture of reasoning processes in the PFC. Using computational modeling and neuroimaging, we show here that the human PFC has two concurrent inferential tracks: (i) one from ventromedial to dorsomedial PFC regions that makes probabilistic inferences about the reliability of the ongoing behavioral strategy and arbitrates between adjusting this strategy versus exploring new ones from long-term memory, and (ii) another from polar to lateral PFC regions that makes probabilistic inferences about the reliability of two or three alternative strategies and arbitrates between exploring new strategies versus exploiting these alternative ones. The two tracks interact and, along with the striatum, realize hypothesis testing for accepting versus rejecting newly created strategies.
Human reasoning combines inferential and creative processes. [Also see Perspective by Hare]
Human reasoning combines inferential and creative processes. [Also see Perspective by Hare]},
	number = {6191},
	journal = {Science},
	author = {Donoso, Maël and Collins, Anne GE and Koechlin, Etienne},
	year = {2014},
	pmid = {24876345},
	pages = {1481--1486},
	file = {Snapshot:C\:\\Users\\maria\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\a3wwptsq.default\\zotero\\storage\\9PI6MDI2\\1481.html:text/html}
}

@article{gershman_gradual_2013,
	title = {Gradual extinction prevents the return of fear: implications for the discovery of state},
	volume = {7},
	issn = {1662-5153},
	shorttitle = {Gradual extinction prevents the return of fear},
	doi = {10.3389/fnbeh.2013.00164},
	abstract = {Fear memories are notoriously difficult to erase, often recovering over time. The longstanding explanation for this finding is that, in extinction training, a new memory is formed that competes with the old one for expression but does not otherwise modify it. This explanation is at odds with traditional models of learning such as Rescorla-Wagner and reinforcement learning. A possible reconciliation that was recently suggested is that extinction training leads to the inference of a new state that is different from the state that was in effect in the original training. This solution, however, raises a new question: under what conditions are new states, or new memories formed? Theoretical accounts implicate persistent large prediction errors in this process. As a test of this idea, we reasoned that careful design of the reinforcement schedule during extinction training could reduce these prediction errors enough to prevent the formation of a new memory, while still decreasing reinforcement sufficiently to drive modification of the old fear memory. In two Pavlovian fear-conditioning experiments, we show that gradually reducing the frequency of aversive stimuli, rather than eliminating them abruptly, prevents the recovery of fear. This finding has important implications for theories of state discovery in reinforcement learning.},
	journal = {Frontiers in Behavioral Neuroscience},
	author = {Gershman, Samuel Joseph and Jones, Carolyn E. and Norman, Kenneth A. and Monfils, Marie-H. and Niv, Yael},
	year = {2013},
	keywords = {extinction, Memory, pavlovian fear conditioning, reinstatement, spontaneous recovery},
	file = {Full Text PDF:C\:\\Users\\maria\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\a3wwptsq.default\\zotero\\storage\\BEHBSJR3\\Gershman et al. - 2013 - Gradual extinction prevents the return of fear im.pdf:application/pdf}
}

@article{alexander_parallel_1986,
	title = {Parallel {Organization} of {Functionally} {Segregated} {Circuits} {Linking} {Basal} {Ganglia} and {Cortex}},
	volume = {9},
	doi = {10.1146/annurev.ne.09.030186.002041},
	number = {1},
	journal = {Annual Review of Neuroscience},
	author = {Alexander, GE and DeLong, MR and Strick, PL},
	year = {1986},
	pmid = {3085570},
	pages = {357--381},
	file = {Full Text PDF:C\:\\Users\\maria\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\a3wwptsq.default\\zotero\\storage\\MIEJC462\\G E Alexander et al. - 1986 - Parallel Organization of Functionally Segregated C.pdf:application/pdf}
}

@article{frank_by_2004,
	title = {By {Carrot} or by {Stick}: {Cognitive} {Reinforcement} {Learning} in {Parkinsonism}},
	volume = {306},
	copyright = {American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	shorttitle = {By {Carrot} or by {Stick}},
	doi = {10.1126/science.1102941},
	abstract = {To what extent do we learn from the positive versus negative outcomes of our decisions? The neuromodulator dopamine plays a key role in these reinforcement learning processes. Patients with Parkinson's disease, who have depleted dopamine in the basal ganglia, are impaired in tasks that require learning from trial and error. Here, we show, using two cognitive procedural learning tasks, that Parkinson's patients off medication are better at learning to avoid choices that lead to negative outcomes than they are at learning from positive outcomes. Dopamine medication reverses this bias, making patients more sensitive to positive than negative outcomes. This pattern was predicted by our biologically based computational model of basal ganglia–dopamine interactions in cognition, which has separate pathways for “Go” and “NoGo” responses that are differentially modulated by positive and negative reinforcement.
A model of learning that incorporates both negative and positive feedback by dopamine explains contradictory findings that dopamine can both improve and hinder cognitive function in patients with Parkinson's disease.
A model of learning that incorporates both negative and positive feedback by dopamine explains contradictory findings that dopamine can both improve and hinder cognitive function in patients with Parkinson's disease.},
	number = {5703},
	journal = {Science},
	author = {Frank, Michael J. and Seeberger, Lauren C. and O'Reilly, Randall C.},
	year = {2004},
	pmid = {15528409},
	pages = {1940--1943},
	file = {Snapshot:C\:\\Users\\maria\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\a3wwptsq.default\\zotero\\storage\\CVETGI73\\1940.html:text/html}
}